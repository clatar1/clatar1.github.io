<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>Halyun Jeong</title>
  </head>
  <body>
    <h1>Halyun Jeong</h1>
    <p> <br>
      University of California, Los Angeles <br>
      Email: hajeong@math.ucla.edu
    <p>I am an assistant adjunct professor in the mathematics department at UCLA. My research mentor is Professor 
<a href="https://www.math.ucla.edu/~deanna/"> Deanna Needell</a>. 
Previously, I was a PIMS postdoc at the University of British Columbia working with Ozgur Yilmaz, Yaniv Plan, and Michael Friedlander. I received my Ph.D. degree in Mathematics from 
Courant Institute of Mathematical Sciences at New York University in 2017.

    <h2>Research</h2>
    <p> My research interests span mathematical aspect of signal processing and machine learning including geometry of high-dimensional data sets, nonlinear signal recovery such as compressed sensing, and computationally efficient optimization. 

As a postdoc at UBC, I worked on concentration of random matrices on sets, performance analysis of iterative algorithms for one-bit compressed sensing, and manifold identification properties of proximal gradient methods and gauge-dual based algorithms. 

For my Ph.D. thesis, I studied phase retrieval, and quantization of phaseless measurements, and analyzed a randomized A/D conversion algorithm that eliminates spectral artifacts. 
       </p>
    <h2>Publications</h2>
<ul>

   <li>
   Stochastic gradient descent for streaming linear and rectified linear systems with Massart noise, Submitted   
    <a href="https://arxiv.org/pdf/2403.01204"> [arXiv link] </a> 
<br/>
(joint work with Deanna Needell and 
<a href="https://erebrova.github.io"> Elizaveta Rebrova</a>)<br/>
   </li>
   <br/>

   <li>
   Nearly Optimal Bounds for Cyclic Forgetting,  Neural Information Processing Systems (NeurIPS), 2023 
<br/>
(joint work with 
<a href="https://www.math.ucla.edu/~markkong"> Mark Kong</a>, 
<a href="https://wswartworth.github.io"> William Swartworth </a>, Deanna Needell, and 
<a href="https://www.rachelward.site"> Rachel Ward</a>)<br/>
   </li>
   <br/>

   <li>
    Linear Convergence of Reshuffling Kaczmarz Methods With Sparse Constraints, Submitted <a href="https://arxiv.org/pdf/2304.10123.pdf"> [arXiv link] </a> 
<br/>
(joint work with Deanna Needell)<br/>
   </li>
   <br/>

   <li>
    Federated Gradient Matching Pursuit, to appear in IEEE Transactions on Information Theory  <a href="https://arxiv.org/pdf/2302.10755.pdf"> [1st version arXiv link] </a> 
<br/>
(joint work with Deanna Needell and 
<a href="https://ms.uky.edu/~jqi229/"> Jing Qin</a>)<br/>
   </li>
   <br/>

   <li>
   Polar Deconvolution of mixed signals, IEEE Transactions on Signal Processing, 2022 <a href="https://ieeexplore.ieee.org/document/9783095"> [journal link] </a> 
<br/>
(joint work with Zhenan Fan, Babhru Joshi, and Michael P. Friedlander)<br/>
   </li>
   <br/>

  <li>
NBIHT: An Efficient Algorithm for 1-bit Compressed Sensing with Optimal Error Decay Rate, IEEE Transactions on Information Theory, 2022  <a href="https://ieeexplore.ieee.org/document/9597562"> [journal link] </a>
<br/>
(joint work with Michael P. Friedlander, Yaniv Plan, and Ozgur Yilmaz)<br/>
</li>
   <br/>

   <li>
Sub-Gaussian Matrices on Sets:  Optimal Tail Dependence and Applications, Communications on Pure and Applied Mathematics, 2021 <a href="https://onlinelibrary.wiley.com/doi/10.1002/cpa.22024?af=R"> [journal link] </a>
<br/>
(joint work with Xiaowei Li, Yaniv Plan, and Ozgur Yilmaz)<br/>

   </li>
   <br/>

  
    <li> Atomic Decomposition Via Polar Alignment: The Geometry of Structured Optimization, Foundations and Trends in Optimization, Volume 3:280-366, 2020 <a href="https://www.nowpublishers.com/article/Details/OPT-028"> [journal link] </a> <a href="Polar_Alignment.pdf"> [pdf] </a>
<br/>
(joint work with Zhenan Fan, Michael P. Friedlander, and Yifan Sun)<br/>
    </li>
   <br/>

    <li> Non-Gaussian Random Matrices on Sets: Optimal Tail Dependence and Applications, Proceedings of International Conference on Sampling Theory and Applications (SampTA), 2019  
<br/>
(joint work with Xiaowei Li, Yaniv Plan, and Ozgur Yilmaz)<br/>
    </li>
   <br/>
 
    <li>  Are we there yet? Manifold identification of gradient-related proximal methods, Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), 2019 
    <a href="http://www.optimization-online.org/DB_FILE/2019/03/7109.pdf"> link. </a>
<br/>
(joint work with  Yifan Sun, Julie Nutini, and Mark Schmidt)<br/>
</li> 
   <br/>

      <li> Convergence of the randomized Kaczmarz method for phase retrieval, 
      <a href="https://arxiv.org/abs/1706.10291"> Preprint. </a>
<br/>
(joint work with  Sinan Gunturk)<br/>

<!--
         </li>
      <li>Halyun Jeong, Thang Huynh, and C. Sinan Güntürk, Distributed
        noise-shaping quantization for phase retrieval, In preparation. 
      </li>
      <li>Halyun Jeong, Spectral analysis of ΣΔ modulation with
        dithering, In preparation.</li>
      <li> Halyun Jeong and Young-Han Kim, <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5205585"> Sparse linear representation </a>,
        Proceedings of the IEEE International Conference on Symposium on
        Information Theory (ISIT) - volume 1, 2009, pp. 329–333 </a>
arXiv: <a href="https://arxiv.org/abs/0905.1990">0905.1990 </a><p></li>
-->
    </ul>

    <h2>Future Talks in 2024</h2>
<ul>
   <li>
[May 2024] Invited talk for SIAM Conference on Applied Linear Algebra 24 minisymposium. 
   </li>
   <li>
[April 2024] Invited talk for the Applied Mathematics Seminar at the University of California, Irvine. 
   </li>
</ul>
    <h2>Teaching at UCLA</h2>
<ul> 
     Math156 Machine Learning  </p>
     Math151B Numerical methods  </p>
     Math170E Probability and Statistics: Probability  </p>
     Math170S Probability and Statistics: Statistics  </p>
</ul>
    <h2>Teaching at UBC</h2>
<ul> 
    <p>Winter 2019 Term 2: <a href="MATH307_2019W2.html">
     Math307 (Applied linear algebra) </a> </p>
    <p>Winter 2018 Term 2: <a href="MATH221_2018W2.html">
     Math221 (Matrix algebra) </a> </p>
    <p>Winter 2018 Term 1: <a href="Math307_2018W1.html">
     Math307 (Applied linear algebra) </a> </p>
    <p>Winter 2017 Term 1: <a href="Math307Winter.html">
     Math307 (Applied linear algebra) </a> </p>
</ul>
    <h2>Teaching at NYU</h2>
<ul> 
    <p>Fall 2016: Calculus 1 recitation</p>
    <p>Fall 2015: Honors III (Fourier analysis) recitation </p>
    <p>Fall 2014: Algebra and Calculus (Precalculus) recitation</p>
</ul>
  </body>
<a/html>
